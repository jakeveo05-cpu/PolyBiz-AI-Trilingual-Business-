# ü§ñ H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t AI cho PolyBiz

PolyBiz h·ªó tr·ª£ 3 ch·∫ø ƒë·ªô AI:

| Ch·∫ø ƒë·ªô | M√¥ t·∫£ | Ph√π h·ª£p v·ªõi |
|--------|-------|-------------|
| **API** | D√πng cloud AI (Gemini, OpenAI, Claude) | M√°y y·∫øu, c·∫ßn ch·∫•t l∆∞·ª£ng cao |
| **Local** | D√πng AI ch·∫°y tr√™n m√°y (Ollama, LM Studio) | M√°y m·∫°nh, mu·ªën mi·ªÖn ph√≠ & ri√™ng t∆∞ |
| **Hybrid** | K·∫øt h·ª£p c·∫£ hai | T·ªëi ∆∞u chi ph√≠ & hi·ªáu su·∫•t |

---

## üöÄ B·∫Øt ƒë·∫ßu nhanh

### 1. Copy file config

```bash
cp config/ai_config.example.json config/ai_config.json
```

### 2. Ch·ªçn ch·∫ø ƒë·ªô

M·ªü `config/ai_config.json` v√† ƒë·∫∑t `mode`:

```json
{
  "mode": "hybrid"  // "api", "local", ho·∫∑c "hybrid"
}
```

---

## ‚òÅÔ∏è Ch·∫ø ƒë·ªô API (Cloud)

### Gemini (Khuy√™n d√πng - MI·ªÑN PH√ç)

1. L·∫•y API key t·∫°i: https://makersuite.google.com/app/apikey
2. C·∫≠p nh·∫≠t config:

```json
{
  "mode": "api",
  "api_providers": {
    "gemini": {
      "enabled": true,
      "api_key": "YOUR_API_KEY",
      "model": "gemini-1.5-flash"
    }
  }
}
```

### Groq (MI·ªÑN PH√ç, c·ª±c nhanh)

1. ƒêƒÉng k√Ω t·∫°i: https://console.groq.com
2. C·∫≠p nh·∫≠t config:

```json
{
  "api_providers": {
    "groq": {
      "enabled": true,
      "api_key": "YOUR_GROQ_KEY",
      "model": "llama-3.1-70b-versatile"
    }
  }
}
```

### OpenAI / Claude (Tr·∫£ ph√≠)

```json
{
  "api_providers": {
    "openai": {
      "enabled": true,
      "api_key": "sk-...",
      "model": "gpt-4o-mini"
    },
    "claude": {
      "enabled": true,
      "api_key": "sk-ant-...",
      "model": "claude-3-haiku-20240307"
    }
  }
}
```

---

## üíª Ch·∫ø ƒë·ªô Local (Open Source)

### Ollama + Qwen (Khuy√™n d√πng cho ti·∫øng Trung)

**B∆∞·ªõc 1: C√†i Ollama**

```bash
# Windows (PowerShell)
winget install Ollama.Ollama

# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh
```

**B∆∞·ªõc 2: T·∫£i model Qwen**

```bash
# Ch·ªçn theo RAM c·ªßa b·∫°n:
ollama pull qwen2.5:3b    # 4GB RAM - nh·∫π
ollama pull qwen2.5:7b    # 8GB RAM - c√¢n b·∫±ng (khuy√™n d√πng)
ollama pull qwen2.5:14b   # 16GB RAM - t·ªët h∆°n
ollama pull qwen2.5:32b   # 32GB RAM - r·∫•t t·ªët
```

**B∆∞·ªõc 3: Ch·∫°y Ollama**

```bash
ollama serve
```

**B∆∞·ªõc 4: C·∫≠p nh·∫≠t config**

```json
{
  "mode": "local",
  "local_providers": {
    "ollama": {
      "enabled": true,
      "endpoint": "http://localhost:11434",
      "model": "qwen2.5:7b"
    }
  }
}
```

### LM Studio (GUI d·ªÖ d√πng)

1. T·∫£i t·∫°i: https://lmstudio.ai
2. T·∫£i model t·ª´ trong app (t√¨m "qwen" ho·∫∑c "llama")
3. B·∫≠t "Local Server" trong app
4. C·∫≠p nh·∫≠t config:

```json
{
  "local_providers": {
    "lmstudio": {
      "enabled": true,
      "endpoint": "http://localhost:1234/v1",
      "model": "local-model"
    }
  }
}
```

---

## ‚ö° Ch·∫ø ƒë·ªô Hybrid (Khuy√™n d√πng)

K·∫øt h·ª£p local cho t√°c v·ª• ƒë∆°n gi·∫£n, API cho t√°c v·ª• ph·ª©c t·∫°p:

```json
{
  "mode": "hybrid",
  
  "api_providers": {
    "gemini": {
      "enabled": true,
      "api_key": "YOUR_KEY",
      "model": "gemini-1.5-flash"
    }
  },
  
  "local_providers": {
    "ollama": {
      "enabled": true,
      "endpoint": "http://localhost:11434",
      "model": "qwen2.5:7b"
    }
  },
  
  "hybrid_rules": {
    "use_local_for": [
      "quick_translation",
      "vocabulary_lookup",
      "simple_grammar_check",
      "flashcard_generation"
    ],
    "use_api_for": [
      "essay_writing",
      "complex_grammar_explanation",
      "mnemonic_creation",
      "conversation_practice"
    ],
    "fallback_order": ["local", "api"],
    "auto_switch_on_error": true
  }
}
```

---

## üîß Models khuy√™n d√πng

### Cho ti·∫øng Trung

| Model | RAM | Ch·∫•t l∆∞·ª£ng | Ghi ch√∫ |
|-------|-----|------------|---------|
| `qwen2.5:7b` | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | T·ªët nh·∫•t cho ti·∫øng Trung |
| `qwen2.5:3b` | 4GB | ‚≠ê‚≠ê‚≠ê | Nh·∫π, v·∫´n t·ªët |
| `glm4:9b` | 12GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Zhipu AI, r·∫•t t·ªët |
| `yi:9b` | 12GB | ‚≠ê‚≠ê‚≠ê‚≠ê | 01.AI, ƒëa ng√¥n ng·ªØ |

### ƒêa nƒÉng

| Model | RAM | Ch·∫•t l∆∞·ª£ng | Ghi ch√∫ |
|-------|-----|------------|---------|
| `llama3.1:8b` | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Meta, ƒëa nƒÉng |
| `phi3:medium` | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Microsoft, nh·ªè g·ªçn |
| `gemma2:9b` | 12GB | ‚≠ê‚≠ê‚≠ê‚≠ê | Google, m·ªõi nh·∫•t |
| `mistral:7b` | 8GB | ‚≠ê‚≠ê‚≠ê | Nhanh, ·ªïn ƒë·ªãnh |

---

## üß™ Test k·∫øt n·ªëi

```bash
python utils/ai_connector.py
```

Output mong ƒë·ª£i:
```
ü§ñ PolyBiz AI Connector Test

üìä Status:
{
  "mode": "hybrid",
  "local_providers": ["ollama"],
  "api_providers": ["gemini"]
}

üîç Checking providers...
  ‚úÖ local/ollama
  ‚úÖ api/gemini

üí¨ Test chat:
Â•Ω (h«éo) l√† m·ªôt ch·ªØ H√°n r·∫•t ph·ªï bi·∫øn...
```

---

## ‚ùì FAQ

**Q: M√°y t√¥i y·∫øu, n√™n d√πng g√¨?**
A: D√πng mode `api` v·ªõi Gemini (mi·ªÖn ph√≠) ho·∫∑c Groq (mi·ªÖn ph√≠, nhanh).

**Q: T√¥i mu·ªën ho√†n to√†n offline?**
A: D√πng mode `local` v·ªõi Ollama + qwen2.5:3b (ch·ªâ c·∫ßn 4GB RAM).

**Q: Model n√†o t·ªët nh·∫•t cho ti·∫øng Trung?**
A: Qwen2.5 c·ªßa Alibaba - ƒë∆∞·ª£c train v·ªõi l∆∞·ª£ng l·ªõn d·ªØ li·ªáu ti·∫øng Trung.

**Q: Hybrid ho·∫°t ƒë·ªông th·∫ø n√†o?**
A: T√°c v·ª• ƒë∆°n gi·∫£n (d·ªãch nhanh, tra t·ª´) ‚Üí Local (mi·ªÖn ph√≠, nhanh)
   T√°c v·ª• ph·ª©c t·∫°p (vi·∫øt lu·∫≠n, gi·∫£i th√≠ch ng·ªØ ph√°p) ‚Üí API (ch·∫•t l∆∞·ª£ng cao)

---

## üîó Links h·ªØu √≠ch

- [Ollama](https://ollama.com) - Ch·∫°y LLM local
- [LM Studio](https://lmstudio.ai) - GUI cho local LLM
- [Google AI Studio](https://makersuite.google.com) - Gemini API key
- [Groq Console](https://console.groq.com) - Groq API key (free)
- [OpenRouter](https://openrouter.ai) - Nhi·ªÅu model, 1 API
